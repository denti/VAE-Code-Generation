{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4394,
     "status": "ok",
     "timestamp": 1538646600554,
     "user": {
      "displayName": "Д Т",
      "photoUrl": "",
      "userId": "06014340589754113358"
     },
     "user_tz": -180
    },
    "id": "xJCZRmNEbUaK",
    "outputId": "f0ad9e7a-7b20-4b2c-fb15-68c6a2e411c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n"
     ]
    }
   ],
   "source": [
    "# Colab only:\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SfOz2oWnbbM1"
   },
   "outputs": [],
   "source": [
    "# Colab only:\n",
    "!pip3 install -q gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n1LQ883IbgmM"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from scipy.stats import norm\n",
    "import nltk.data\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import reuters\n",
    "from nltk. corpus import gutenberg\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Input, Dense, Lambda, Layer, LSTM, RepeatVector, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics, objectives\n",
    "import nltk\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 651,
     "status": "ok",
     "timestamp": 1538646604843,
     "user": {
      "displayName": "Д Т",
      "photoUrl": "",
      "userId": "06014340589754113358"
     },
     "user_tz": -180
    },
    "id": "vyW7a9Zubo76",
    "outputId": "80947419-9c87-4c76-d7ec-4134964719fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Colab only:\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16GkzF30fAIP"
   },
   "source": [
    "\n",
    "### Открываем исходники,склеенные при помощи строки \"@...\". Разделяем и токенизируем исходники при помощи nltk. Представляем коды в виде массивов последовательностей токенов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13260,
     "status": "ok",
     "timestamp": 1538647250185,
     "user": {
      "displayName": "Д Т",
      "photoUrl": "",
      "userId": "06014340589754113358"
     },
     "user_tz": -180
    },
    "id": "s_gBNxcdcAAC",
    "outputId": "6e0941c0-9ada-4773-b95a-9a8f75ec38bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens examples:  ['/*', 'PR', 'preprocessor/41445', '*/', '/*', '{', 'dg-do', 'compile', '}', '*/', '/*', '{', 'dg-options', '``', '-gdwarf', '-O0', '-dA', '-fno-merge-debug-strings', \"''\", '}']\n"
     ]
    }
   ],
   "source": [
    "file_content = open(\"./data/sources.txt\").read()\n",
    "file_content += \"GOODEXAMPLESTART\" + \" | \" + \"BADEXAMPLESTART\" + \" | \" +  \"EXAMPLESTOP\" \n",
    "file_content_arr = file_content.split(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "\n",
    "tokenized = []\n",
    "for fa in file_content_arr:\n",
    "   tokenized.append(nltk.word_tokenize(fa))\n",
    "\n",
    "print(\"Tokens examples: \", tokenized[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBONVus-gEeK"
   },
   "source": [
    "### Растягиваем в 100мерном пространстве последовательности токенов при помощи Word2Vec. Так же добавляем в модель w2v отдельностоящие старт/стоп слова для генирации послдовательностей GOODEXAMPLESTART/ BADEXAMPLESTART/EXAMPLESTOP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36287,
     "status": "ok",
     "timestamp": 1538647307085,
     "user": {
      "displayName": "Д Т",
      "photoUrl": "",
      "userId": "06014340589754113358"
     },
     "user_tz": -180
    },
    "id": "hFoDssd3cBia",
    "outputId": "adc3b311-c4b6-45e3-b12d-01bc806c224b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# w2v_model = Word2Vec.load('./models/w2v_cpp_model.h5')\n",
    "\n",
    "\n",
    "# W2V model train:\n",
    "w2v_model = Word2Vec(tokenized, size=100, window=5, min_count=1, workers=4)\n",
    "w2v_model.train(tokenized, total_examples=len(tokenized), epochs=10)\n",
    "w2v_model.wv.syn0[w2v_model.wv.vocab[\"GOODEXAMPLESTART\"].index] = np.ones((1,100)).flatten()\n",
    "w2v_model.wv.syn0[w2v_model.wv.vocab[\"EXAMPLESTOP\"].index] = np.zeros((1,100)).flatten()\n",
    "tmp = np.zeros((1,100)).flatten()\n",
    "tmp[::2]=1\n",
    "w2v_model.wv.syn0[w2v_model.wv.vocab[\"BADEXAMPLESTART\"].index] = tmp\n",
    "# w2v_model.save('./models/w2v_cpp_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1538647313159,
     "user": {
      "displayName": "Д Т",
      "photoUrl": "",
      "userId": "06014340589754113358"
     },
     "user_tz": -180
    },
    "id": "IcklckFAcDHC",
    "outputId": "8a77f394-cf47-4105-8516-3c540d01a026"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:1366: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.vectors_norm = (self.vectors / sqrt((self.vectors ** 2).sum(-1))[..., newaxis]).astype(REAL)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('j', 0.7125623822212219),\n",
       " (';', 0.6415524482727051),\n",
       " ('0', 0.6092534065246582),\n",
       " ('10', 0.5986620783805847),\n",
       " ('16', 0.5569596290588379),\n",
       " ('k', 0.5469374656677246),\n",
       " ('dst', 0.5397963523864746),\n",
       " ('tot', 0.5363589525222778),\n",
       " ('=', 0.5350142121315002),\n",
       " ('sum1', 0.5293612480163574)]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similar_by_word('i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mE4FTCQohuy7"
   },
   "source": [
    "### Урезаем количество последовательностей для теста работоспособности модели в Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HgBgSNzcFH5"
   },
   "outputs": [],
   "source": [
    "# We are cutting tokenized array due to memory limit\n",
    "tokenized = tokenized[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-C0tUIJ6iiiO"
   },
   "source": [
    "### Векторизуем последовательности токенов, чтобы вместо последовательностей слов/символов кода получить последовательности векторов из w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 568,
     "status": "ok",
     "timestamp": 1538647581800,
     "user": {
      "displayName": "Д Т",
      "photoUrl": "",
      "userId": "06014340589754113358"
     },
     "user_tz": -180
    },
    "id": "ILJEE9uYcG5h",
    "outputId": "fc0026ac-78e7-41b5-b617-b3cba739e6a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_word_vec (100,)\n",
      "stop_word_vec (100,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# Cooking sequences for seq2seq\n",
    "\n",
    "# Encoder sequence to w2v seq\n",
    "us_encoder_input_data = []\n",
    "\n",
    "for seq in tokenized:\n",
    "  us_encoder_input_data.append([])\n",
    "  for word in seq:\n",
    "    us_encoder_input_data[-1].append(w2v_model.wv.word_vec(word))\n",
    "  us_encoder_input_data[-1] = np.array(us_encoder_input_data[-1])\n",
    "  \n",
    "  \n",
    "# Decoder sequence to w2v seq\n",
    "start_word = \"GOODEXAMPLESTART\"\n",
    "start_word_vec = w2v_model[start_word]\n",
    "print('start_word_vec', start_word_vec.shape)\n",
    "\n",
    "stop_word = \"EXAMPLESTOP\"\n",
    "stop_word_vec = w2v_model[stop_word]\n",
    "print('stop_word_vec', stop_word_vec.shape)\n",
    "\n",
    "is_first_iteration = True\n",
    "us_decoder_input_data = []\n",
    "us_decoder_target_data = []\n",
    "\n",
    "for seq in tokenized:\n",
    "    us_decoder_input_data.append([])\n",
    "    us_decoder_target_data.append([])\n",
    "    #   start word appends only to decoder_input_data\n",
    "    us_decoder_input_data[-1].append(start_word_vec)\n",
    "    for word in seq:\n",
    "        word_vectorized = w2v_model.wv.word_vec(word)\n",
    "        us_decoder_input_data[-1].append(word_vectorized)\n",
    "        us_decoder_target_data[-1].append(word_vectorized)\n",
    "    #   stop word appends only to decoder_target_data\n",
    "    us_decoder_target_data[-1].append(stop_word_vec)\n",
    "    \n",
    "    us_decoder_input_data[-1] = np.array(us_decoder_input_data[-1])\n",
    "    us_decoder_target_data[-1] = np.array(us_decoder_target_data[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnLLLTcTizwu"
   },
   "source": [
    "### Создаем квадратные матрицы (по максимальным длинам наших массивов последовательностей). Данные матрицы удобны и пригодны для подачи на вход нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 578,
     "status": "ok",
     "timestamp": 1538647805754,
     "user": {
      "displayName": "Д Т",
      "photoUrl": "",
      "userId": "06014340589754113358"
     },
     "user_tz": -180
    },
    "id": "mxw1i1gBcIc6",
    "outputId": "6ddb3ca0-1644-48cc-e35e-80594ac51d54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length=0\n",
    "for a in us_decoder_input_data:\n",
    "  if max_sentence_length < a.shape[0]:\n",
    "    max_sentence_length = a.shape[0]\n",
    "print(max_sentence_length)\n",
    "\n",
    "encoder_input_data = np.zeros((\n",
    "    len(us_encoder_input_data),\n",
    "    max_sentence_length,\n",
    "    100,\n",
    "))\n",
    "decoder_input_data = np.zeros(encoder_input_data.shape)\n",
    "decoder_target_data = np.zeros(encoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ij3TvRw8jN9r"
   },
   "source": [
    "### Копируем векторизованные последовательности в матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1955,
     "status": "ok",
     "timestamp": 1538647814603,
     "user": {
      "displayName": "Д Т",
      "photoUrl": "",
      "userId": "06014340589754113358"
     },
     "user_tz": -180
    },
    "id": "zUcPC95ncR8B",
    "outputId": "1c559f81-e18d-459c-b7f0-503e035c6e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 328, 100) (100, 328, 100) (100, 328, 100)\n"
     ]
    }
   ],
   "source": [
    "# matricize sequences\n",
    "for i, seq in enumerate(us_encoder_input_data):\n",
    "  for j, word in enumerate(seq):\n",
    "    for h in range(100):\n",
    "      encoder_input_data[i,j,h] = us_encoder_input_data[i][j,h]\n",
    "      \n",
    "      \n",
    "for i, seq in enumerate(us_decoder_input_data):\n",
    "  for j, word in enumerate(seq):\n",
    "    for h in range(100):\n",
    "      decoder_input_data[i,j,h] = us_decoder_input_data[i][j,h]\n",
    "      decoder_target_data[i,j,h] = us_decoder_target_data[i][j,h]\n",
    "      \n",
    "print(encoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cz3E8EiCjXQg"
   },
   "source": [
    "### Задаем параметры нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quQs0EpycTRw"
   },
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "epsilon_std=0.01\n",
    "batch_size = 100  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = encoder_input_data.shape[0]  # Number of samples to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UsLTlQy3lLc4"
   },
   "source": [
    "### Создаем нейросеть, это обычная Seq2Seq модель нейросети, взятая из блога Keras (https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)\n",
    "### Наша модель учится генерировать те же самые последовательности, которые были поданы ей на вход.\n",
    "\n",
    "### Но при этом, на выходе Encoder-а мы заворачиваем его стейты (state_h, state_c) в нейросеть с Variational слоями. Эти слои взяти из примера Mnist VAE (https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py)\n",
    "\n",
    "### При помощи такого хода мы можем выучить распределение слоев  (state_h, state_c) и, в дальнейшем, беря из этого распределения сэмпл, и, пропуская его через декодер, мы можем получить на выходе из декодера рандомно сгенеренную последовательность из интересующего нас распределения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17125,
     "status": "error",
     "timestamp": 1538653821285,
     "user": {
      "displayName": "Д Т",
      "photoUrl": "",
      "userId": "06014340589754113358"
     },
     "user_tz": -180
    },
    "id": "tTjyR21PcVEH",
    "outputId": "1f22ee24-3314-4982-faa3-b95f22f75f02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 7.0034 - val_loss: -3.0129\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-71693e57941e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m           validation_split=0.2)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Seq2Seq + VAE model\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    x = K.flatten(x)\n",
    "    x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "    xent_loss = decoder_input_data.shape[2] * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + sd_h - K.square(mn_h) - K.exp(sd_h), axis=-1)\n",
    "    kl_loss -= 0.5 * K.mean(1 + sd_c - K.square(mn_c) - K.exp(sd_c), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "  \n",
    "# Define an input sequence and process it.\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None, decoder_input_data.shape[2]))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Original Variational layer from Mnist vae:\n",
    "mn = tf.layers.dense(x, units=n_latent)\n",
    "sd       = 0.5 * tf.layers.dense(x, units=n_latent)            \n",
    "epsilon = tf.random_normal(tf.stack([tf.shape(x)[0], n_latent])) \n",
    "z  = mn + tf.multiply(epsilon, tf.exp(sd))\n",
    "\n",
    "\"\"\"\n",
    "# Learn distribution from Hidden state\n",
    "# Layer were incapsulated into Lambda: \n",
    "mn_h = Dense(latent_dim)(state_h)\n",
    "sd_h = Lambda(lambda x: tf.multiply(0.5, Dense(latent_dim)(x)))(state_h)\n",
    "epsilon_h =  Lambda(lambda x: tf.random_normal(tf.stack([tf.shape(x)[0], latent_dim])))(state_h) \n",
    "z_h = Lambda(lambda args: tf.add(args[0], tf.multiply(args[1], tf.exp(args[2]))))([mn_h, epsilon_h, sd_h])\n",
    "\n",
    "# Learn distribution from Cell state\n",
    "# Layers were incapsulated with Lambda: \n",
    "mn_c = Dense(latent_dim)(state_c)\n",
    "sd_c = Lambda(lambda x: tf.multiply(0.5, Dense(latent_dim)(x)))(state_c)\n",
    "epsilon_c = Lambda(lambda x: tf.random_normal(tf.stack([tf.shape(x)[0], latent_dim])))(state_c) \n",
    "z_c  = Lambda(lambda args: tf.add(args[0], tf.multiply(args[1], tf.exp(args[2]))))([mn_c, epsilon_c, sd_c])\n",
    "\n",
    "encoder_states = [z_h, z_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, decoder_input_data.shape[2]))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(decoder_input_data.shape[2], activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVwJN18IvBzT"
   },
   "source": [
    "### Описываем модель генератора рандомных последовательностей кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRaff0JAcWWP"
   },
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_2Eu8s2vIW0"
   },
   "source": [
    "### Описываем функцию, которая циклически токен за токеном генерирует последовательность кода при помощи модели генератора.\n",
    "\n",
    "### Основное отличие от обычной функции генерации из модели Seq2Seq в том, что мы подаем на вход декодеру стейты, взятые из выученного распределения (вместо стейтов, взятых на выходе энкодера)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vv7QD704qESr"
   },
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean_, z_log_var_ = args\n",
    "    batch_size = K.shape(z_mean_)[0]\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=epsilon_std)\n",
    "    return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "#     states_value = encoder_model.predict(input_seq)\n",
    "    states_value = [np.random.normal(0, 1, latent_dim).reshape((1, latent_dim)) for _ in range(2)]\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, 100))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = w2v_model.wv['GOODEXAMPLESTART']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = w2v_model.wv.similar_by_vector(output_tokens[0, -1, :])[0][0]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == 'EXAMPLESTOP' or\n",
    "           len(decoded_sentence) > max_sentence_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, 100))\n",
    "        target_seq[0, 0] = w2v_model.wv['GOODEXAMPLESTART']\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2093,
     "status": "ok",
     "timestamp": 1538653866467,
     "user": {
      "displayName": "Д Т",
      "photoUrl": "",
      "userId": "06014340589754113358"
     },
     "user_tz": -180
    },
    "id": "RZIoCBLVqF3D",
    "outputId": "4d91f3ba-3b4c-4128-c5c9-41f4b8822517"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Decoded sentence: GOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTART\n",
      "-\n",
      "Decoded sentence: GOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTARTGOODEXAMPLESTART\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(2):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "#     print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OAZR1ehd6sFD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "KasperskyCodeGen_Seq2Seq_VAE.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
